%%!TEX TS-program = latex

\documentclass[12pt]{article}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{mdwlist}
\geometry{a4paper} % or letter
\geometry{margin=1in}

\title{ PaxosLease: Diskless Paxos for Leases }
\author{ Marton Trencseni, \texttt{mtrencseni@scalien.com} \and
		 Attila Gazso, \texttt{agazso@scalien.com} \and
		 Holger Reinhardt, \texttt{hreinhardt@gmail.com}
}
\date{}

\begin{document}

\maketitle


\section{ Introduction }
%%%%%%%%%%%%%%%%%%%%%%%%

In concurrent programming, \emph{locks} are a basic primitive which processes use to synchronize access to a shared resource. In a system where locks are granted without an automatic expiry time (and without an overseer process), failure of the lock owner before it releases the lock may cause all other processes to block.

In a highly-available distributed system, one wishes to avoid single node failure to cause the entire system to block. Also, "restarting" the system in case of failure may be harder than for a multi-threaded program. Thus, in distributed systems, \emph{leases} take the place of locks to avoid starvation. \emph{A lease is a lock with an automatic expiry time.} If the owner of the lock fails or becomes disconnected from the rest, its lease automatically expires and other nodes can acquire it.

We assume the basic set up is as follows: the system consists of a set of \emph{poposers} who follow the proposer's algorithm and a set of \emph{acceptors} who follow the acceptor's algorithm. It is assumed the system is non-byzantine, ie. the nodes do not cheat (are not hacked) by not following their algorithm. The number of acceptors is fixed and does not change.

A naive, majority vote type algorithm can be given which correctly solves the distributed lease problem, \emph{correct} meaning that the lease is held by no more than one node at any time. However, this simple algorithm will frequently \emph{block} in the presence of many proposers, hence the need for a more sophisticated approach. 

The naive majority algorithm is as follows: proposers start local timeouts for $T$ seconds and send requests to the acceptors for the lease with timespan $T$. The acceptors, upon receiving the request, start a timer for $T$ seconds and sends an accept message to the proposer. After the timeout, the acceptors clear their state. If an acceptor receives a request but its state is not empty it either does not answer or sends a reject message. To make sure only one proposer has the lease at any one time, the proposer must receive accept messages from a majority of acceptors; then it has the lease until its local timer expires.

As discussed previously, it is possible (and likely), that with many proposers, noone will be able to get a majority and proposers will continually block each other. For example, with three proposers 1, 2 and 3 and three acceptors A, B and C, if the distributed state is such that A accepts 1's request, B 2's and C 3's, then no proposer has a majority. The system must wait until the timeouts expire and the acceptors discard their state at which point the proposers will try again. However, it is likely that the system will block again.

The solution described in this paper is to follow the scheme of Paxos \cite{Parliament}, and introduce \emph{prepare} and \emph{propose} phases, which avoids this kind of blocking altogether\footnote{Another solution may be to let the system block, but introduce an "undo" mechanism which lets some proposers undo their proposals to let one proposer acquire the lease.}. Paxos solves the problem of replicated state-machines, where each node has a local copy of a state-machine, and they wish to reach consensus on the next state transition. Paxos is a majority based algorithm, meaning a majority of the nodes have to be up and communicating for consensus to be possible. Paxos deals with reaching consensus on a single state transition, so in practice several Paxos rounds are run in sequence to negotiate the sequence of state transitions \cite{PaxosMadeLive}. In Paxos, acceptors record their state to disk before sending responses, which guarantees that once a value (state transition) is chosen, it is chosen at all later times; in other words, all state machines go through the same sequence of state transitions, whatever error conditions occur.

Unlike earlier Paxos-based distributed lease algorithms such as Fatlease \cite{Fatlease}, PaxosLease does not make any time synchrony assumptions about local clocks of nodes (no global synchronization with a bounded skew is required). Also, Fatlease runs Paxos rounds in succession for lease commands, whereas PaxosLease makes use of the temporal nature of leases and avoids this complication altogether for a much simpler and elegant algorithm.

PaxosLease is a natural specialization of Paxos. As in Paxos, it is assumed that the number of nodes is fixed (and node identities are globally known). PaxosLease deals with a special replicated state-machine of the form:

\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.8]{distr_state.eps}
\caption{PaxosLease distributed state machine.}
\label{default}
\end{center}
\end{figure}

To acquire the lease, PaxosLease proposer nodes propose the value: "node i has the lease", and the return to the "noone has the lease" state is automatic after the expiry time. Proposers may also extend their leases by proposing the "node i has the lease" value again before their previous lease expires, and may optionally release the lease before it expires.

Similar to Paxos, PaxosLease intrinsically handles all relevant failure conditions:
\begin{enumerate*}
\item nodes stop and restart
\item network splits
\item message loss and reordering
\item in-transit message delays
\end{enumerate*}


\section{ Definitions }
%%%%%%%%%%%%%%%%%%%%%%%

A PaxosLease cell consists of proposers and acceptors. We assume there are $n$ acceptors and any number of proposers. In practice, nodes often act as proposers and acceptors, but this is an implementation issue and does not affect the discussion.

Proposers send \emph{prepare request} and \emph{propose request} messages to acceptors, who reply with \emph{prepare response} and \emph{propose response} messages. The messages have the following structure:
\begin{enumerate*}
\item prepare request = ballot number
\item prepare response = ballot number, answer, accepted proposal
\item propose request = ballot number, lease
\item prepare response = ballot number, answer
\end{enumerate*}

A ballot number and a lease together make up a \emph{proposal}. A \emph{lease} is made up of a proposer id (who wishes to become the lease owner) and a timespan $T$.

Acceptors store the following state information:
\begin{enumerate*}
\item highest ballot number promised: acceptors ignore messages whose ballot number is less than this
\item accepted proposal: the last proposal (ballot number and lease) accepted
\end{enumerate*}

There is a globally known maximal lease time $M$. Proposers always acquire leases for timespans $T < M$.
%Proposers guarantee to use different ballot numbers (between each other) and not to-reuse ballot numbers with time interval $M$, the maximal lease time. In practice, this means ballot numbers are divided into two bitfields: one identifies the proposer, the other in a monotonically increasing per-proposer counter which may be reset when the proposer restarts.

Tt is important that ballot numbers be globally unique and monotonously increasing per proposer. In practice, this can is achieved by ballot numbers being made up of a \emph{proposer id} field, a \emph{restart counter} field and a counter (at the most significant end) specific to this run of the proposer. The restart counter is incremented each time the proposer starts up and is written to stable storage.

PaxosLease preserves the \emph{lease-invariant}: at any given time, there is no more than one proposer which holds the lease.

\section{ Basic algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%

This section describes the basic flow of the algorithm in terms of the proposer's and acceptor's algorithm. The proposers send prepare and propose requests, acceptors reply with prepare and propose responses. If all goes well, it takes two round-trip times for the proposer to acquire the lease.

\begin{enumerate}

\item A proposer wishes to acquire the lease for $T < M$ seconds. It generates a ballot number, starts a timer which will expire in $T$ seconds and sends prepare request messages to a majority of acceptors.

\item Acceptors, upon receiving a prepare request check whether the ballot number is higher than their local highest ballot number promised. If it is lower, they either discard the message or send a prepare response with a \emph{reject} answer. If it equal or higher, the acceptor constructs a prepare response with an \emph{accept} answer containing the currently accepted proposal, which may be \emph{empty}. The acceptor sets its highest ballot number promised equal to the ballot number of the message and sends the prepare response back to the proposer.

\item The proposer examines the prepare responses from the acceptors. If a majority of acceptors responded with empty proposals, meaning they are open to accept new proposals, the proposer is free to propose itself as the lease owner for time $T$. It sends propose requests containing the ballot number and the lease (its proposer id and $T$). 

\item Acceptor, upon receiving a propose request check whether the ballot number is higher than their highest ballot number promised. If it is lower, they can either discard the message or send a propose response with a \emph{reject} answer. If it equal or higher, the acceptor accepts the proposal: if starts a timeout which will expire in $T$ seconds and sets its accepted proposal to the proposal received (if it has stored a previous proposal, it discards it). It then constructs a propose response with an \emph{accept} answer containing the ballot number. After the timeout expires the acceptor resets its accepted proposal to \emph{empty}. Acceptors never reset their highest ballot number promised, except when restarting.

\item The proposer examines the propose responses messages. If a majority of acceptors responded accepting the proposal, the proposer has the lease until its local timeout, started in step 3. expires. The time at which the proposer receives the last message necessary for a majority is the time at which it acquired the lease and may switch its internal state to "I have the lease".

\end{enumerate}

Note that acceptors do not store their state to stable storage. Upon restart, they start out in a blank state. To make sure restarting nodes do not violate the lease invariant, nodes wait $M$ seconds before rejoining the network. $M$ is a globally known maximal lease time known to all nodes, and proposers always acquire leases for $T < M$ seconds.

\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.8]{timeflow.eps}
\caption{Time flow of a proposer acquiring the lease.}
\label{default}
\end{center}
\end{figure}

It is important to note that since all that is transmitted is timespans (relative times), only the proposer which has the lease knows it has the lease. It cannot tell other nodes it has the lease (similar to learn messages is classical Paxos), since other nodes cannot know how much time these learn messages spent in transit. Thus, only the proposer which has the lease knows it has the lease. All other nodes know is that they do not have the lease. In other words, each proposer has two states regarding the lease: "I do not have the lease and I don't know who has the lease" and "I have the lease". Of course, nodes may send out learn messages as \emph{hints} which may be used in higher-level applications or heuristics, but this is beyond the scope of this paper.

It is possible that a proposer cannot get a majority of acceptors to send favorable responses in steps 3. and 5. above. In this case the proposer may sleep for a while and then restart its algorithm at step 1. with a higher ballot number.


\section{ Proof of lease invariance }
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

First we give the reader an intuitive notion of why PaxosLease works. Figure 2. (above) is a pictorial explanation: the proposer starts its timer before sending out prepare requests, acceptors only start their timer \emph{later}, before sending out propose responses, thus, if a majority of acceptors stored the state and started timers, no other proposer will be able to get the lease before the proposer's timer expires. No two proposers will simultaneously believe themselves to be lease holders. 

More formallny, PaxosLease guarantees that if proposer $i$ receives propose accept messages from a majority of acceptors for its proposal with ballot number $b$ and timespan $T$ at time $t_{now}$, then if it started its timer at time $t_{start}$, no other proposer will receive a similar majority until $t_{end} = t_{start} + T$.

Proof: Suppose proposer $p$ with ballot number $b$ acquired the lease. It started its timer at $t_{start}$, received prepare responses of type $accept$ from a majority of acceptors at $t_{acquire}$, and thus has the lease until $t_{end} = t_{start} + T$. Let $A_1$ be the majority of acceptors who replied with empty prepare responses to $p$'s prepare requests and $A_2$ be a majority of acceptors who accepted $p$'s proposal (sent prepare responses of type \emph{accept}).

Part 1. No other proposer $p'$ can hold the lease between $t_{acquire}$ and $t_{end}$ with ballot number $b' < b$. In order to hold the lease, the proposer $p'$ must get a majority $A_2'$ of acceptors to accept its lease. Let $a$ be an acceptor who is in both $A_2'$ and $A_1$: since $b' < b$, $a$ must have first accepted $p'$'s proposal and then sent a prepare response to $p$. However, if $a$ sent an empty prepare response to $p$, its state must have been empty, so it could not have previously accepted $p'$'s lease.

Part 2. No other proposer $p'$ can hold the lease between $t_{acquire}$ and $t_{end}$ with ballot number $b < b'$. In order to hold the lease, the proposer $p'$ must get a majority $A_1'$ of acceptors to send it empty prepare responses. Let $a$ be an acceptor who is in both $A_1'$ and $A_2$: since $b < b'$, $a$ must have first accepted $p$'s proposal and then sent a prepare response to $p'$. However, since $a$ accepted $p$'s proposal, it could not have sent an empty prepare response to $p'$.

\section{ Proof of liveness }

\section{ Extending leases }

In some situations, once a proposer has the lease, it is important that it can hold on to it beyond its original lease time. A typical case is whent the lease identifies the master node in a distributed system and it is desirable that a node can hold on to its mastership for a long time.

To accomodate this only the proposer's algorithm needs to be modified. In step 3., if a majority responded either with empty proposals \emph{or its existing} proposal whose lease has not yet expired, it can propose itself again as the lease holder. This will allow the proposer to extend its lease by $O(T)$ time. The acceptor's algorithm does not change.

\section{ Releasing leases }
%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In the algorithm described so far, the proposerss lease automatically expire after some time. In some situations, it is important to release the lease as soon as possible so that other nodes may acquire it. A typical case is a distributed filesystem, where the proposers acquire leases for a file, write to it, and then wish to release the lease as asson as possible so that other writers can acquire it.

To accomodate this, any proposer may at any point in time send special release messages to acceptors which contain the ballot number of the lease they wish to release. Before sending out the release messages, the proposer swtiches its internal state from "I have the lease" to "I do not have the lease". If an acceptor receives a release message, it checks whether its accepted ballot number matches. If if does, it discards its state; otherwise it does nothing. Proposers can also send out release messages to other proposers as hints, telling them that they may be able to acquire the lease now.

\section{ Leases for many resources }
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The algorithm defines lease actions concerning a single resource $R$. In practice nodes may deal with many resources, such as write locks for files in a distributed filesystem. PaxosLease can be used by running independent instances of the algorithm for each resource by tagging each message, proposer and acceptor state with a \emph{resource identifier}. A node acting as proposer and acceptor requires no more than $\sim 100$ bytes of memory per PaxosLease instance, which means it can handle $\sim 10$ million resource leases per gigabyte of main memory. This and the fact that PaxosLease does not require disk syncs or time synchronization means the algorithm may be used in a wide variety of scenarios for fine-grained locking.

\section{ Implementation}
%%%%%%%%%%%%%%%%%%%%%%%%%

\section{ Genealogy }
%%%%%%%%%%%%%%%%%%%%%%%%%%%

Leslie Lamport invented Paxos in 1990, but it was only published in 1998. This paper, \emph{'The Part-Time Parliament'} was "greek to many readers", which led to a second paper, \emph{'Paxos Made Simple'}. Paxos was popularized by its use in Google's in-house distributed stack, described in \emph{'Paxos Made Live - An Engineering Perspective'} and \emph{'The Chubby Lock Service for Loosely-Coupled Distributed Systems'}.

\begin{thebibliography}{9}

\bibitem{Parliament}
L. Lamport, \emph{The Part-Time Parliament}, ACM Transactions on Computer Systems 16, 2 (May 1998), 133-169.

\bibitem{PaxosMadeSimple}
L. Lamport, \emph{Paxos Made Simple}, ACM SIGACT News 32, 4 (Dec. 2001), 18Ð25.

\bibitem{PaxosMadeLive}
T. Chandra, R. Griesemer, J. Redstone, \emph{Paxos Made Live - An Engineering Perspective}, PODC '07: 26th ACM Symposium on Principles of Distributed Computing

\bibitem{Chubby}
M. Burrows, \emph{The Chubby Lock Service for Loosely-Coupled Distributed Systems}, OSDI'06: Seventh Symposium on Operating System Design and Implementation.

\bibitem{Fatlease}
F. Hupfeld et al., \emph{FaTLease: Scalable Fault-Tolerant Lease Negotiation with Paxos}, HPDCÕ08, June 23Ð27, 2008, Boston, Massachusetts, USA.


\end{thebibliography}

\end{document}